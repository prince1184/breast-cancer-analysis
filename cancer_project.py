# -*- coding: utf-8 -*-
"""Cancer_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r1z_NQMgkpdYxCF96AziWuFkoS6ipEOT

### Breast Cancer Analysis using Python
"""

#Guys you can copy this project
#just go to files then click on save a copy in drive

# Commented out IPython magic to ensure Python compatibility.
# import necessary libraries to perform analysis
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn import tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

cancer_data_set = pd.read_csv("breast_cancer_data_set.csv")

print(cancer_data_set)

"""# **Data set Understanding**

To check top rows aruguments given 15, if its default it takes 5 rows

# **Displaying Head**
"""

cancer_data_set.head(10)

"""To check bottom rows aruguments given 15, if its default it takes 5 rows

# **Displaying Tail**
"""

cancer_data_set.tail(10)

"""Key Feature from Data frame head

id Column: contain the unique ids, therefore cannot be used for classification.

𝑑𝑖𝑎𝑔𝑜𝑛𝑠𝑖𝑠 column: With the binary Values → Target column containing the class 𝑙𝑎𝑏𝑒𝑙𝑠

𝑀 - 𝑀𝑎𝑙𝑖𝑔𝑛𝑎𝑛𝑡 - Tending to invade normal tissue

𝐵 - 𝐵𝑒𝑛𝑖𝑔𝑛 - not harmful in effect.

# **Data Acquisition**
**Displaying Type**
"""

print(cancer_data_set.dtypes)# to check the data type of the data set

"""**Displaying Null**

to check missing value in the data set, if data is missing we have to do correction, in this case there is no data missing isnull() will show true or false, if sum() added it will show in count
"""

cancer_data_set.isnull().sum()

"""**Display Size**

to count the number of elements along a given axis

"""

print("size of the DataFrame: ")
print(cancer_data_set.size)

"""The describe() method returns description of the data in the DataFrame. If the DataFrame contains numerical data, the description contains these information for each column count - The number of not-empty values. mean - The average (mean) value"""

cancer_data_set.describe()

"""**Displaying shape**

"""

#shape that returns a tuple with each index having the number of corresponding elements

print("Shape of DataFrame: ")
print(cancer_data_set.shape)

"""**Displaying Dimension**"""

# to check the length of each dimention
print("Number of Dimensions:")
print(cancer_data_set.ndim)

"""# **Data Prepartion**

**Data preparation**
"""

# Creating the Target Class
# Storing the Class label or Target in --> y (M or B)
y_target = cancer_data_set.diagnosis

# Making a list of unwanted columns
list = ['id','diagnosis']

# Dropping the unnecessary Column
data = cancer_data_set.drop(list,axis = 1 ) # Dropping Column `axis = 1`, for rows `axis = 0`
data.head(10)

"""**Creating the Pairplot from Seaborn**

"""

"""
Create dataset for finding contribution of individual features towards whether
or not a certain cancer tumor is malignant or benign.
"""

# Map values in diagnosis column, 0 representing benign  and 1 represeting malignant
cancer_data_set['diagnosis'] = cancer_data_set['diagnosis'].map({'B': 0, 'M': 1})
data_for_corr = cancer_data_set[['radius_mean', 'perimeter_mean', 'area_mean',
                               'compactness_mean', 'concavity_mean',
                               'concave points_mean', 'diagnosis']]
#Create data_for_corr with various features and diagnosis
data_for_corr.head()

"""**B→0**

**M→1**

# **Data Exploration**

Pairplot from Seaborn to see relationship between individual features and diagnosis


**Pairplot**
"""

sns.pairplot(data_for_corr, palette='coolwarm',hue= 'diagnosis')

"""**Separating and discretizing**

Now we have the two data frames: for the class labels y other for the features x

**Count plot**
"""

# checking the distribution for Target Varible using seaborn library:
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)
ax = sns.countplot(y_target, label="Count") # countplot: tells us the count of each class in column.
B, M = y_target.value_counts() # Using -`value_counts` from the Pandas to store the individual count.
print('Number of Benign tumors : ', B)
print('Number of Malignant tumors : ', M)

data.describe() # Pandas:- descriptive statistics

"""# **Data Visualization**

**Visualizing Standardized Data with Seaborn**

Here some values are hundreds, intense and some are in zero points, very differenticated data, data that connot ploted easily so we have to standerdise it, should have similar standards to plot and understand to perform subtract data from its mean again devide it from data standard
"""

# first ten features

x_features = data
data_n = (data - data.mean()) / (data.std())  # data normalization for plotting

# get all the features -- since axis = 1, Columnwise Concatenation
data_vis = pd.concat([y_target, data_n.iloc[:,0:10]], axis=1)

# let's flat the dataset
# `pd.melt` -- Unpivot the given DataFrame from wide format to long format
# it Massages a DataFrame into a right format
data_vis = pd.melt(data_vis, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(20,10))


sns.violinplot(x = "features",
               y = "value",
               hue = "diagnosis",
               data = data_vis,
               split = True,
               inner = "quart"
               )

plt.xticks(rotation=45); # --matplotlib--

"""Interpreting the above violin plot in the texture_mean the median value of Malignent & banign are well seprated as compare to fractal_dimension_worst where the median of two Coincide with each other, this means that this could be a good value feature for classification.



**Violin Plots and Box Plots**
"""

# Second ten features
data_vis = pd.concat([y_target, data_n.iloc[:,10:20]], axis=1)

data_vis = pd.melt(data_vis, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(20,10))

sns.violinplot(x = "features",
               y = "value",
               hue = "diagnosis",
               data = data_vis,
               split = True,
               inner = "quart")

plt.xticks(rotation=45); # --matplotlib--

# Third ten features
data_vis = pd.concat([y_target, data_n.iloc[:,20:31]], axis=1)
data_vis = pd.melt(data_vis, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(20,10))

sns.violinplot(x = "features",
               y = "value",
               hue = "diagnosis",
               data = data_vis,
               split = True,
               inner = "quart")

plt.xticks(rotation=45); # --matplotlib--

"""Interpreting the above violin plot: Concavity_worst & Concave points_worst looks very similar, So how can we decide that are they co-related with each other or not, if they are co-related with each other then the best practice is to reduce the redundancey by dropping one of the column."""

# As an alternative of violin plot, box plot can be used
# box plots are also useful in terms of seeing outliers
# I do not visualize all features with box plot
# In order to show you lets have an example of box plot
# If you want, you can visualize other features as well.
plt.figure(figsize=(20,10))
sns.boxplot(x="features",
            y="value",
            hue="diagnosis",
            data=data_vis)

plt.xticks(rotation=45);

"""BoxPlot are good alternative when we want to detect the outliers.

**Using Joint Plots for Feature Comparison**
"""

# Checking how co-related the two feature are
sns.jointplot(x=x_features.loc[:,'concavity_worst'],
              y=x_features.loc[:,'concave points_worst'],
              kind="reg")

"""By looking the above joint plot we can say that these two feature are highly Co-related.

**Data Wrangling univatiate filters**
"""

sns.set(style="white")
df = x_features.loc[:,['radius_worst','perimeter_worst','area_worst']]
g = sns.PairGrid(df, diag_sharey=False)
g.map_lower(sns.kdeplot, cmap="Blues_d")
g.map_upper(plt.scatter)
g.map_diag(sns.kdeplot, lw=3);

"""**Observing the Distribution of Values and their Variance with Swarm Plots**"""

# First Ten features

import warnings
warnings.simplefilter(action="ignore", category=UserWarning)
sns.set(style="whitegrid", palette="muted")
data_dia = y_target #data diameter

data_n = (data - data.mean()) / (data.std())

data_vis = pd.concat([y_target, data_n.iloc[:,0:10]],axis=1)
data_vis = pd.melt(data_vis, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(20,10))
sns.swarmplot(x="features",
              y="value",
              hue="diagnosis",
              data=data_vis)

plt.xticks(rotation=45);

# Second ten features

import warnings
warnings.simplefilter(action="ignore", category=UserWarning)

data_vis = pd.concat([y_target, data_n.iloc[:,10:20]],axis=1)
data_vis = pd.melt(data_vis, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(20,10))
sns.swarmplot(x="features",
              y="value",
              hue="diagnosis",
              data=data_vis)

plt.xticks(rotation=45);

# Third ten features

import warnings
warnings.simplefilter(action="ignore", category=UserWarning)

data_vis = pd.concat([y_target, data_n.iloc[:,20:31]],axis=1)
data_vis = pd.melt(data_vis, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(20,10))
sns.swarmplot(x="features",
              y="value",
              hue="diagnosis",
              data=data_vis)

plt.xticks(rotation=45);

#A swarm plot can be drawn on its own, but it is also a good complement to a box or violin plot in cases
#where you want to show all observations along with some representation of the underlying distribution.

#So by looking at the variance of swarm plot we can tell how well seprated they are, & which features are best suitable for classification.

"""**how good we predict the target using correlation features**"""

# Correlation of each feature and our target variable

data_vis = pd.concat([y_target, data_n.iloc[:,0:30]],axis=1)
data_vis = pd.melt(data_vis, id_vars="diagnosis",
                    var_name="features",
                    value_name='value')

plt.figure(figsize=(30,20))
sns.swarmplot(x="features",
              y="value",
              hue="diagnosis",
              data=data_vis)

plt.xticks(rotation=90);

"""This plot provides a comprehensive overview of the relationship between each feature and the diagnosis in the breast cancer dataset. By analysing this plot, we can see that some features clearly differentiate between the malignant and benign diagnoses, such as 'radius_mean', 'area_mean', and 'concavity_mean'. Other features, such as 'smoothness_mean', 'symmetry_mean', and 'fractal_dimension_mean', do not show as clear of a difference between the two diagnoses.

**Observing all Pair-wise Correlations**

**Data Wrangling Heatmap**




*  Each square shows the correlation between the variables on each axis. Correlation ranges from -1 to +1.
*   Values closer to zero means there is no linear trend between the two variables.


*   The close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is.

*   A correlation closer to -1 is similar, but instead of both increasing one variable will decrease as the other increases.

*   The diagonals are all 1 because those squares are correlating each variable to itself (so it's a perfect correlation).
*   For the rest the larger the number and darker the color the higher the correlation between the two variables. The plot is also symmetrical about the diagonal since the same two variables are being paired together in those squares.
"""

f, ax = plt.subplots(figsize=(25, 25))
sns.heatmap(x_features.corr(), annot=True, linewidths=.5, fmt='.1f', ax=ax, cmap="YlGnBu")
plt.show()

"""**white cell → Highly Correlated**

**Black Cell → Non-correlated**

In this heat map light color values shows that are highly co realted, where as dark color are shows that not much co related 0.9 and its light and highly co realated. concavity_worst and concavity_mean, are very much co realated With 0.3 negetive value, radius_worst and fractal_dimention_mean are not corelated and its useful for further analysis

# **Data Wrangling Decision Tree Classifier**
"""

x_train, x_test, y_train, y_test = train_test_split(data_n.iloc[:,0:30], y_target, test_size=0.3, random_state=42)

clf = tree.DecisionTreeClassifier(random_state=42)
clf.fit(x_train, y_train)

# Evaluate the classifier on the testing set
y_pred = clf.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, pos_label='M')
recall = recall_score(y_test, y_pred, pos_label='M')
f1 = f1_score(y_test, y_pred, pos_label='M')

fig = plt.figure(figsize=(20,10))
tree.plot_tree(clf, filled=True)
[...]

x_train, x_test, y_train, y_test = train_test_split(data_n.iloc[:,0:30], y_target, test_size=0.3, random_state=42)

clf = tree.DecisionTreeClassifier(random_state=42)
clf.fit(x_train, y_train)

# Evaluate the classifier on the testing set
y_pred = clf.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, pos_label='M')
recall = recall_score(y_test, y_pred, pos_label='M')
f1 = f1_score(y_test, y_pred, pos_label='M')

fig = plt.figure(figsize=(20,10))
tree.plot_tree(clf, filled=True)
[...]

#evaluation metrics of the classifier on the testing set

print('Accuracy: {:.3f}'.format(accuracy))
print('Precision: {:.3f}'.format(precision))
print('Recall: {:.3f}'.format(recall))
print('F1-score: {:.3f}'.format(f1))

"""This indicates that the classifier has an accuracy of 0.918, meaning that it correctly predicts the class label for 91.8% of the samples in the testing set. The precision, recall, and F1-score for the malignant class are 0.910, 0.907, and 0.909, respectively, indicating that the classifier is relatively good at identifying malignant tumors."""